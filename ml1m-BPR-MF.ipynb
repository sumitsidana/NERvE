{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1::1193::5::978300760\r\n",
      "1::661::3::978302109\r\n",
      "1::914::3::978301968\r\n",
      "1::3408::4::978300275\r\n",
      "1::2355::5::978824291\r\n",
      "1::1197::3::978302268\r\n",
      "1::1287::5::978302039\r\n",
      "1::2804::5::978300719\r\n",
      "1::594::4::978302268\r\n",
      "1::919::4::978301368\r\n"
     ]
    }
   ],
   "source": [
    "! head ./data/ml-1m/ratings.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from rank_metrics import ndcg_at_k\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt('./data/ml-1m/ratings.dat', delimiter='::')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TriplesDataset(object):\n",
    "    \n",
    "    def __init__(self, raw_data, threshold_user = 20, threshold_item = 5, rnd_seed = 42):\n",
    "        self.threshold_user = threshold_user\n",
    "        self.threshold_item = threshold_item\n",
    "        \n",
    "        self.N_ITEMS = int(max(raw_data[:, 1]))\n",
    "        \n",
    "        # fix randomness\n",
    "        self.seed = rnd_seed\n",
    "        np.random.seed(rnd_seed)\n",
    "        random.seed(rnd_seed)\n",
    "        \n",
    "        # calculate user/item counts\n",
    "        u_cnt = {}\n",
    "        i_cnt = {}\n",
    "        for row in tqdm(raw_data, desc='Calculate counts', leave=False):\n",
    "            u, i, r, t = row.astype(int)\n",
    "            u_cnt[u] = u_cnt.get(u, 0) + 1\n",
    "            i_cnt[i] = i_cnt.get(i, 0) + 1\n",
    "\n",
    "        # make blacklists\n",
    "        u_blacklist = set([k for k, v in u_cnt.items() if v < threshold_user])\n",
    "        i_blacklist = set([k for k, v in i_cnt.items() if v < threshold_item])\n",
    "        \n",
    "        # assemble self.data like {user:[(item, rating),...], ...}\n",
    "        self.data = {}\n",
    "        self.used = {}\n",
    "        for row in tqdm(raw_data, desc='Assemble .data', leave=False):\n",
    "            u, i, r, t = list(map(int, row))\n",
    "            # filtering rare\n",
    "            if u in u_blacklist or i in i_blacklist:\n",
    "                continue\n",
    "            self.data[u] = self.data.get(u, []) + [(i, r)]\n",
    "            self.used[u] = self.used.get(u, []) + [i]\n",
    "        self.data_keys = list(self.data.keys())\n",
    "        self.train_pool = None\n",
    "    \n",
    "    \n",
    "    def train_test_split(self, n_train=None, n_test=None):\n",
    "        assert n_train is None or n_test is None, 'both n_train and n_test cannot be set'\n",
    "        #assert not (n_train is None and n_test is None), 'both n_train and n_test cannot be None'\n",
    "        assert n_test is None, 'n_test not supported for now'\n",
    "        \n",
    "        # .train : {user: {rating: [items, ...], ...}}\n",
    "        self.train = {}\n",
    "            \n",
    "        # .test : {user: [(item, rating), ..]} [sorted by rating]\n",
    "        self.test = {}\n",
    "            \n",
    "        for u in tqdm(self.data, desc='Split users', leave=False):\n",
    "            rnd_inds = set(np.random.choice(len(self.data[u]), n_train or n_test, False))\n",
    "            for n, pair in enumerate(self.data[u]):\n",
    "                if n in rnd_inds:\n",
    "                    i, r = pair\n",
    "                    user_dict = self.train.get(u, {})\n",
    "                    rating_list = user_dict.get(r, [])\n",
    "                    rating_list.append(i)\n",
    "                    user_dict[r] = rating_list\n",
    "                    self.train[u] = user_dict\n",
    "                else:\n",
    "                    self.test[u] = self.test.get(u, []) + [pair]\n",
    "            if len(self.train[u].keys())==1:\n",
    "                print('No rating diversity in train for user {}, do swap!'.format(u))\n",
    "                the_only_rating = list(self.train[u].keys())[0]\n",
    "                for n, (i, r) in enumerate(self.test[u]):\n",
    "                    if r!=the_only_rating:\n",
    "                        self.train[u][r] = [i]\n",
    "                        extracted_i = self.train[u][the_only_rating][0]\n",
    "                        self.train[u][the_only_rating] = self.train[u][the_only_rating][1:]\n",
    "                        del self.test[u][n]\n",
    "                        self.test[u] = self.test[u] + [(extracted_i, the_only_rating)]\n",
    "                        break\n",
    "            self.test[u] = sorted(self.test[u], key=lambda x: x[1], reverse=True)\n",
    "                    \n",
    "    def sample_train_triple(self):\n",
    "        user = random.choice(self.data_keys)\n",
    "        stats = self.train[user]\n",
    "        stats_keys = list(stats.keys())\n",
    "        assert len(stats_keys) > 1, 'user {} has only 1 rating!'.format(user)\n",
    "          \n",
    "        # custom sampler from 0 rating -- uses non-labeled data (as implicit feedback)\n",
    "        def sampler(rating, stats, used):\n",
    "            if rating > 0:\n",
    "                return random.choice(stats[rating])\n",
    "            while True:\n",
    "                rnd = random.randint(0, self.N_ITEMS - 1)\n",
    "                if rnd not in used:\n",
    "                    return rnd\n",
    "                \n",
    "        left_rating, right_rating = random.sample(stats_keys + [0], 2)\n",
    "        left_value = sampler(left_rating, stats, self.used[user])\n",
    "        right_value = sampler(right_rating, stats, self.used[user])\n",
    "        y = (left_rating > right_rating)*2 - 1\n",
    "        return (user, left_value, right_value, y)\n",
    "    \n",
    "    \n",
    "    def sample_train_batch(self,n_samples=256):\n",
    "        retval = np.zeros((n_samples, 4)).astype(np.int32)\n",
    "        for i in range(n_samples):\n",
    "            retval[i] = self.sample_train_triple()\n",
    "        return {\n",
    "            'users': retval[:, 0], \n",
    "            'left_items': retval[:, 1],\n",
    "            'right_items': retval[:, 2],\n",
    "            'y': retval[:, 3].astype(np.float32),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = TriplesDataset(raw_data, threshold_user=30, rnd_seed=42)\n",
    "ds.train_test_split(n_train=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_USERS = int(max(raw_data[:, 0])) + 1\n",
    "N_ITEMS = int(max(raw_data[:, 1])) + 1\n",
    "N_HIDDEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# placeholders\n",
    "user_ids  = tf.placeholder(tf.int32, (None,), name='user_ids')\n",
    "left_ids  = tf.placeholder(tf.int32, (None,), name='left_ids')\n",
    "right_ids = tf.placeholder(tf.int32, (None,), name='right_ids')\n",
    "target_y  = tf.placeholder(tf.float32, (None,), name='target_y')\n",
    "\n",
    "\n",
    "# main parameters\n",
    "user_latents = tf.Variable(tf.random_uniform(shape=(N_USERS, N_HIDDEN)), trainable=True, name='user_latents')\n",
    "item_latents = tf.Variable(tf.random_uniform(shape=(N_ITEMS, N_HIDDEN)), trainable=True, name='item_latents')\n",
    "\n",
    "## define batch processing\n",
    "\n",
    "# get embeddings for batch\n",
    "embedding_user  = tf.nn.embedding_lookup(user_latents, user_ids, name='embedding_user')\n",
    "embedding_left  = tf.nn.embedding_lookup(item_latents, left_ids, name='embedding_left')\n",
    "embedding_right = tf.nn.embedding_lookup(item_latents, right_ids, name='embedding_right')\n",
    "\n",
    "# raw margins for primal ranking loss\n",
    "embedding_diff = embedding_left - embedding_right\n",
    "\n",
    "# shape: [n_batch, ]\n",
    "embedding_margins = tf.reduce_sum(tf.mul(embedding_user, embedding_diff), axis=1, name='embedding_margins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# losses\n",
    "def mean_logloss(raw_margins, target_values, tag):\n",
    "    # -y*f(x)\n",
    "    myfx = -1*tf.mul(target_values, raw_margins)\n",
    "    elementwise_logloss = tf.minimum(tf.log(1 + tf.exp(myfx)), 100, name='elwise_' + tag)\n",
    "    mean_loss = tf.reduce_mean(elementwise_logloss, name='mean_' + tag)\n",
    "    return mean_loss\n",
    "\n",
    "embedding_loss = mean_logloss(embedding_margins, target_y, 'embedding_loss')\n",
    "target = embedding_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer_2 = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(target)\n",
    "trainer_3 = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(target)\n",
    "trainer_4 = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_vars = tf.global_variables_initializer()\n",
    "sess.run(init_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "batch_size = 1024\n",
    "for n_batches, cur_optim in [(3000, trainer_2)]:\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        batch = ds.sample_train_batch(n_samples=batch_size)\n",
    "        fd = {\n",
    "            user_ids:  batch['users'], \n",
    "            left_ids:  batch['left_items'],\n",
    "            right_ids: batch['right_items'],\n",
    "            target_y:  batch['y'],\n",
    "        }\n",
    "        el, _ = sess.run([embedding_loss, cur_optim], feed_dict=fd)\n",
    "        losses.append((el, 0.5))\n",
    "        if i%1000==0:\n",
    "            user_norm = np.linalg.norm(user_latents.eval())\n",
    "            item_norm = np.linalg.norm(item_latents.eval())\n",
    "            print('Weight norms, users: {}, items: {}'.format(user_norm, item_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot([x[0] for x in losses], c='b', label='embedding_loss', alpha=0.5)\n",
    "plot([x[1] for x in losses], c='r', label='net_loss', alpha=0.5)\n",
    "grid()\n",
    "legend()\n",
    "xlabel('n_batches')\n",
    "ylabel('logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = ds.sample_train_batch(n_samples=1)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = {\n",
    "    user_ids:  batch['users'], \n",
    "    left_ids:  batch['left_items'],\n",
    "    right_ids: batch['right_items'],\n",
    "    target_y:  np.array([1]),\n",
    "}\n",
    "    \n",
    "u, l, r, d, m, t = sess.run([embedding_user, embedding_left, embedding_right, embedding_diff, embedding_margins, target], fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndcg_vals = []\n",
    "auc_vals = []\n",
    "for u in tqdm(ds.data_keys, desc='Prediction', leave=True):\n",
    "    fd = {\n",
    "            user_ids:  (np.ones(N_ITEMS)*u).astype(np.int32), \n",
    "            left_ids:  np.array(range(N_ITEMS)).astype(np.int32),\n",
    "        }\n",
    "    response = sess.run(embedding_left, feed_dict=fd)[:, 0]\n",
    "\n",
    "    \n",
    "    \n",
    "    # make response\n",
    "    INF = 10e+5\n",
    "    response[0] = -INF\n",
    "    for rating, items in ds.train[u].items():\n",
    "        for i in items:\n",
    "            response[i] = -INF\n",
    "\n",
    "    # make relevances\n",
    "    relevances = np.zeros(N_ITEMS)\n",
    "    for i, r in ds.test[u]:\n",
    "        relevances[i] = r\n",
    "    \n",
    "    auc_vals.append(roc_auc_score(relevances > 0, response))\n",
    "    \n",
    "    sorted_ids = np.argsort(-response)\n",
    "    relevances = relevances[sorted_ids]\n",
    "    \n",
    "    # calc score\n",
    "    ndcg_vals.append(ndcg_at_k(relevances, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(ndcg_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(auc_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(ndcg_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndcg_at_k(relevances, 10, method=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
